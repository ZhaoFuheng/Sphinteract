{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import collections\n",
    "import ast\n",
    "import time\n",
    "import collections\n",
    "import json\n",
    "import sqlite3\n",
    "import tiktoken\n",
    "import xxhash\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the openai api key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "def save(fname, d):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(d, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "from multiprocessing import Process, Queue\n",
    "import query_module\n",
    "\n",
    "# def handler(signum, frame):\n",
    "#     print('time out')\n",
    "#     raise TimeoutError(\"Query execution exceeded the allowed time limit.\")\n",
    "\n",
    "\n",
    "def evalfunc(sql_source, sql_target, database, source='kaggle'):\n",
    "    assert source in ['kaggle', 'bird']\n",
    "    db_path = ''\n",
    "    if source == 'kaggle':\n",
    "        db_path = f'./databases/{database}/{database}.sqlite'\n",
    "    elif source == 'bird':\n",
    "        db_path = f'./bird_dev/dev_databases/{database}/{database}.sqlite'\n",
    "    if not os.path.isfile(db_path):\n",
    "        print(\"cannot find file\", db_path)\n",
    "        return False\n",
    "    timeout = 120\n",
    "    output = Queue()\n",
    "    query_process = Process(target=query_module.execute_query, args=(db_path, sql_source, output))\n",
    "    query_process.start()\n",
    "    output_hash = ''\n",
    "    \n",
    "    try:\n",
    "        # Connect to sqlite db\n",
    "        # Execute both!\n",
    "        source_results = None\n",
    "        source_results = output.get(True, timeout+5)\n",
    "        query_process.join(timeout)\n",
    "        if query_process.is_alive():\n",
    "            print(\"process terminated\")\n",
    "            query_process.terminate()  # Terminate the process\n",
    "            query_process.join()  # Make sure it's cleaned up\n",
    "            return False, [Exception('SQL query took too much time to execute.')]\n",
    "        if isinstance(source_results, Exception):\n",
    "            raise source_results\n",
    "        output_hash = xxhash.xxh128_hexdigest(str(len(source_results)), seed=123)\n",
    "        connection = sqlite3.connect(db_path)\n",
    "        cursor = connection.cursor()\n",
    "        target_results = cursor.execute(sql_target).fetchall()\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        # If the lengths don't match... there's no hope\n",
    "        if len(source_results) != len(target_results):\n",
    "            # (result matches or not, valid, hash)\n",
    "            return False, []\n",
    "        if 'ORDER BY' in sql_target:\n",
    "            for a, b in zip(source_results, target_results):\n",
    "                # NOTE: we are doing compares that are column-order independent\n",
    "                # hence the sorting and the weird key (since we may have mixed\n",
    "                # types in a row)\n",
    "                lhs = tuple(sorted(list(a), key=lambda x: hash(x)))\n",
    "                rhs = tuple(sorted(list(b), key=lambda x: hash(x)))\n",
    "                output_hash = xxhash.xxh128_hexdigest(output_hash + str(lhs), seed=123)\n",
    "                if lhs != rhs:\n",
    "                    # Oh no, a row doesn't match!\n",
    "                    return False, []\n",
    "        else:\n",
    "            lset, rset = set(), set()\n",
    "            for a, b in zip(source_results, target_results):\n",
    "                # NOTE: we are doing compares that are column-order independent\n",
    "                # hence the sorting and the weird key (since we may have mixed\n",
    "                # types in a row)\n",
    "                lset.add(tuple(sorted(list(a), key=lambda x: hash(x))))\n",
    "                rset.add(tuple(sorted(list(b), key=lambda x: hash(x))))\n",
    "            output_hash = xxhash.xxh128_hexdigest(str(lset), seed=123)\n",
    "            if lset != rset:\n",
    "                # Oh no, rows don't match!\n",
    "                return False, []\n",
    "    # If we hit an error, that's not a match I guess...\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return False, [ex]\n",
    "    return True, []\n",
    "\n",
    "\n",
    "def outputHash(sql_source, database):\n",
    "    db_path = f'./databases/{database}/{database}.sqlite'\n",
    "    output_hash = ''\n",
    "    try:\n",
    "        # Connect to sqlite db\n",
    "        connection = sqlite3.connect(db_path)\n",
    "        cursor = connection.cursor()\n",
    "        source_results = cursor.execute(sql_source).fetchall()\n",
    "        output_hash = xxhash.xxh128_hexdigest(str(len(source_results)), seed=123)\n",
    "        if 'ORDER BY' in sql_source:\n",
    "            for a in source_results:\n",
    "                lhs = tuple(sorted(list(a), key=lambda x: hash(x)))\n",
    "                output_hash = xxhash.xxh128_hexdigest(output_hash + str(lhs), seed=123)\n",
    "        else:\n",
    "            lset = set()\n",
    "            for a in source_results:\n",
    "                lset.add(tuple(sorted(list(a), key=lambda x: hash(x))))\n",
    "            output_hash = xxhash.xxh128_hexdigest(str(lset), seed=123)\n",
    "    except Exception as ex:\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    return output_hash\n",
    "\n",
    "\n",
    "def execute(sql, database, source):\n",
    "    assert source in ['kaggle', 'bird']\n",
    "    db_path = ''\n",
    "    if source == 'kaggle':\n",
    "        db_path = f'./databases/{database}/{database}.sqlite'\n",
    "    elif source == 'bird':\n",
    "        db_path = f'./bird_dev/dev_databases/{database}/{database}.sqlite'\n",
    "        \n",
    "    if not os.path.isfile(db_path):\n",
    "        print(\"cannot find file\")\n",
    "        return False\n",
    "    results = ''\n",
    "    try:\n",
    "        # Connect to sqlite db\n",
    "        connection = sqlite3.connect(db_path)\n",
    "        cursor = connection.cursor()\n",
    "        results = cursor.execute(sql).fetchall()\n",
    "    # If we hit an error, that's not a match I guess...\n",
    "    except KeyboardInterrupt:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"KeyboardInterrupt\")\n",
    "        return False\n",
    "    except Exception as ex:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(ex)\n",
    "        return False\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4_turbo_generation(prompt, t = 0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=t,\n",
    "        max_tokens=4096,\n",
    "        logprobs=True,\n",
    "    )\n",
    "    logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    return response.choices[0].message.content.strip(), perplexity_score\n",
    "\n",
    "def GPT4o_generation(prompt, t = 0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=t,\n",
    "        max_tokens=4096,\n",
    "        logprobs=True,\n",
    "    )\n",
    "    logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    return response.choices[0].message.content.strip(), perplexity_score\n",
    "\n",
    "\n",
    "\n",
    "def GPT35_generation(prompt, t = 0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=t,\n",
    "        max_tokens=4096,\n",
    "        logprobs=True,\n",
    "    )\n",
    "    logprobs = [token.logprob for token in response.choices[0].logprobs.content]\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    return response.choices[0].message.content.strip(), perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kaggle_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_data_df = pd.read_json('bird_dev.json')\n",
    "bird_data_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(bird_data_df) == 1534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "userstudy = []\n",
    "with open('./logs/user_study.pkl', 'rb') as f:\n",
    "    userstudy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "survey_questions = []\n",
    "# add original gold query inside\n",
    "for d in userstudy:\n",
    "    if 'Question2Ask' in d:\n",
    "        assert len(d['Question2Ask']) == len(d['Answer2Question']), print(d['nl'])\n",
    "    q = d['nl']\n",
    "    sql = df.loc[df['nl'] == q]['sql'].values\n",
    "    d['gold'] = sql[0]\n",
    "    d[\"target_schema\"] = df.loc[df['nl'] == q]['target_schema'].values\n",
    "    survey_questions.append(q)\n",
    "\n",
    "print(len(df))\n",
    "# drop the user study questions\n",
    "df = df[~df['nl'].isin(survey_questions)]\n",
    "print(len(df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(sql_query):\n",
    "    sql_query = sql_query.replace(\"```sql\", '')\n",
    "    sql_query = sql_query.replace(\"```\", '')\n",
    "    sql_query = sql_query.replace(';', '')\n",
    "    sql_query = sql_query.replace('\"\"\"', '')\n",
    "    if 'SELECT' not in sql_query.upper()[:10]:\n",
    "        sql_query = 'SELECT ' + sql_query\n",
    "    return sql_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_prefix_v1='''/* some examples are provided */\n",
    "/* example question: */\n",
    "How many acres burned in fires in California each year between 2000 and 2005?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  SUM(FIRE_SIZE),\\n  FIRE_YEAR\\nFROM Fires\\nWHERE\\n  State = \"CA\" AND FIRE_YEAR BETWEEN 2000 AND 2005\\nGROUP BY\\n  FIRE_YEAR\n",
    "/* example clarification question*/\n",
    "What information should the output table contain? a) two columns: the total acres burned and the year, b) one column: the total acres burned for each year, c) one column: the total acres burned across all target years, d) other (please specify).\n",
    "/* example reasoning */\n",
    "Output table is determined by the SELECT clause in the gold sql query. The gold query uses ‘SELECT  SUM(FIRE_SIZE), FIRE_YEAR’. As a result, the output table has two columns, the total acres burned and the year. Hence, choice a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"a) two columns: the total acres burned and the year\"\n",
    "\n",
    "/* example question: */\n",
    "Which states had the largest number of fires in 2001?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  State\\nFROM Fires\\nWHERE\\n  FIRE_YEAR = 2001\\nGROUP BY\\n  State\\nORDER BY\\n  COUNT(*) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Is the largest number of fires referring to? a) the total size of all fire incidents, b) the number of fire incidents, c) the largest size of all fire incidents, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The clarification question is asking about how to represent the largest number of fires. The gold query uses ‘ORDER BY COUNT(*) DESC LIMIT 1’ to find the largest number of fires. As a result, choice a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) the number of fire incidents\"\n",
    "\n",
    "/* example question: */\n",
    "What was the most common cause of fire between 2000 and 2005?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  STAT_CAUSE_DESCR\\nFROM Fires\\nWHERE\\n  FIRE_YEAR BETWEEN 2000 AND 2005\\nGROUP BY\\n  STAT_CAUSE_DESCR\\nORDER BY\\n  COUNT(*) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Which information should be used to represent the 'cause of fire'? a) the code that represents the cause, b) the description of the cause, c) both the code and the description of the cause, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The clarification question is asking for which column should be used to represent the cause of fire. The gold query uses the STAT_CAUSE_DESCR to represent the cause. As a result, choice b is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) the description of the cause\"\n",
    "\n",
    "/* example question: */\n",
    "Whose CDs sells best?\n",
    "/* example gold sql query*/\n",
    "SELECT\\n  artist\\nFROM torrents\\nGROUP BY\\n  artist\\nORDER BY\\n  SUM(totalSnatched) DESC\\nLIMIT 1;\n",
    "/* example clarification question*/\n",
    "Which column should be used to identify music related to 'CD'? a) groupName, b) tag, c) releaseType, d) other (please specify)\n",
    "/* example reasoning */\n",
    "The gold query does not use a WHERE clause to filter the CDs. Hence, the CD information is not contained in the tag column or the release type column. As a result, choice a, b, and c are all wrong.\n",
    "/* example  answer*/\n",
    "answer_to_cq = “d) Consider all music; No filter on ‘CD’ ”\n",
    "\n",
    "/* example question: */\n",
    "How many people wrote comments for the question \"Any additional notes or comments.\"? */\n",
    "/* example gold sql query*/\n",
    "SELECT COUNT(T1.UserID) FROM Answer AS T1 INNER JOIN Question AS T2 ON T1.QuestionID = T2.questionid WHERE T2.questiontext LIKE 'Any additional notes or comments' AND T1.AnswerText IS NOT NULL\n",
    "/* example clarification question*/\n",
    "How to determine if a user has provided comments? a) no check needed, b) see if `AnswerText` column has empty string, c) other (please specify).\n",
    "/* example reasoning */\n",
    "In the gold SQL query, it checks “T1.AnswerText IS NOT NULL”. Hence, choice a and b are both wrong.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"c) ‘wrote comments’ imply `AnswerText` is not a NULL value\".\n",
    "\n",
    "/* example question: */\n",
    "Calculate the difference between the number of customers and the number of subscribers who did the trip in June 2013. \n",
    "/* example gold sql query*/\n",
    "SELECT SUM(IIF(subscription_type = 'Subscriber', 1, 0)) - SUM(IIF(subscription_type = 'Customer', 1, 0)) FROM trip WHERE start_date LIKE '6/%/2013%'\n",
    "/* example clarification question*/\n",
    "What predicate value should be used to determine a trip in June 2013? a) start_data > 06/2013, b) start_data = ‘June 2013’, c) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold sql query uses start_date LIKE '6/%/2013%' to find trips in June 2013.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"c) start_date LIKE '6/%/2013%'\"\n",
    "\n",
    "\n",
    "/* example question: */\n",
    "Identify the players who weigh 120 kg.\n",
    "/* example gold sql query*/\n",
    "SELECT T2.PlayerName FROM weight_info AS T1 INNER JOIN PlayerInfo AS T2 ON T1.weight_id = T2.weight WHERE T1.weight_in_kg = 120\n",
    "/* example clarification question*/\n",
    "What fields should be contained in the output? a) one column of player name, b) one column of player id, c) two columns of player name and player ids, d) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold query selects ‘SELECT T2.PlayerName’. Hence, a is correct.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"a) one column of player name\"\n",
    "\n",
    "/* example question: */\n",
    "How many reviews are created for the podcast \"Scaling Global\" under?\n",
    "/* example gold sql query*/\n",
    "SELECT COUNT(T2.content) FROM podcasts AS T1 INNER JOIN reviews AS T2 ON T2.podcast_id = T1.podcast_id WHERE T1.title = 'Scaling Global'\n",
    "/* example clarification question*/\n",
    "Which column represents the reviews? a) `podcast` column, b) `content` column, c) other (please specify).\n",
    "/* example reasoning */\n",
    "The gold query uses “COUNT(T2.content)” to determine the number of reviews. Hence, b is correct in which the `content` column represents the reviews.\n",
    "/* example  answer*/\n",
    "answer_to_cq = \"b) `content` column\"\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "# feedback_v2 = \"\"\"Your task is to answer the multiple choice clarification question truthfully based on the Gold Query.\n",
    "\n",
    "# Natural Language Question: ```{nlq}```\n",
    "# Gold Query: ```{query}```\n",
    "# The gold query is the sql answer to the natural language question\n",
    "\n",
    "# Multiple Choice Clarification Question: ```{question}```\n",
    "\n",
    "# Answer the above clarification question truthfully based on the Gold Query.\n",
    "\n",
    "# First, reason which portion of the gold query answers the clarification question.\n",
    "# Next, consider the correctness of each multiple choice answers based only on the gold query.\n",
    "# Lastly, output the answer with format: answer_to_cq = \"\".\n",
    "# If none of the multiple choices are correct or you selected `other (please specify)`, give a short answer to the clarification question after `other (please specify)`.\n",
    "\n",
    "\n",
    "# Let's think step by step:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "feedback_v2 = \"\"\"/* Given the following Natural Language Question: */\n",
    "{nlq}\n",
    "/* And the following Gold Query: */\n",
    "{query}\n",
    "/* Answer the following multiple choice clarification question truthfully based on the Gold Query: */\n",
    "{question}\n",
    "\n",
    "/* Follow these steps:\n",
    "1. Identify which portion of the Gold Query answers the clarification question.\n",
    "2. Evaluate the correctness of each multiple choice answer based only on the Gold Query.\n",
    "3. If none of the choices are correct or you select \"other (please specify)\", provide a short answer for the clarification question.\n",
    "4. Output the final answer in the format: answer_to_cq = \"\".\n",
    "\n",
    "Let’s proceed step by step. */\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cq_prefix_v1 = '''/* some examples are provided */\n",
    "/* example question: */\n",
    "Which artist/group is most productive?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: How to rank artist/group productivity? a) rank by the number of records produced, b) rank by the total number of downloads, c) other (please specify).\n",
    "user: b) rank by the total number of downloads```\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer should use ORDER BY and LIMIT 1 based on the sum of total downloads. However, it is unclear what columns should be used to represent the 'artist/group'.  Both the `artist` and the `groupName` columns contain information about 'artist/group'. ’‘AmbTableColumn’ remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = \"Which columns represent the 'artist/group' information? a) the artist column only, b) the groupName column only, c) both the artist column and the groupName column, d) other (please specify).”```\n",
    "\n",
    "/* example question: */\n",
    "Which Premier League matches ended in a draw in 2016?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: Is the year '2016' referring to? a) season is 2016, b) season is either 2015/2016 or 2016/2017, c) the date time is at year 2016, d) other (specify).\n",
    "user: a) season is 2016,\n",
    "clarification questions: How to find the 'Premier league'? a) consider all leagues, b) consider only the league with name 'Premier League', c) other (specify).\n",
    "user: b) consider only the league with name 'Premier League'\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer to this question needs to contain a WHERE clause for three conditions: 'Premier League', 'draw', and 'in 2016'. However, the question did not specify what fields should be contained in the output table. 'AmbOutput' remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “What fields represent the target 'matches'? a) all fields from football data table, b) the `league` column, c) other (specify).”\n",
    "\n",
    "/* example question: */\n",
    "Which type of crime has the highest rate of ‘Investigation complete’?\n",
    "/* example previous clarification questions and user replies: */\n",
    "No previous clarification questions.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the SQL answer to this question needs to contain a WHERE clause to find crimes that have 'Investigation complete' outcomes, uses ORDER BY and LIMIT 1 to find the type of crime with the highest rate, and the output table has only one row. However, it needs to be clarified i) what predicate value should be used for 'Investigation complete', and ii) how to represent the 'rate', and iii) if the output table contains only the crime type column or the crime type column with the highest rate aggregate. Hence, this question is ambiguous because of 'AmbVal', 'AmbQuestion', and ‘AmbOutput’.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “What information should be used to find 'Investigation complete'? a) see if outcome contains the phrase 'Investigation complete', b)  see if outcome is 'Investigation complete; no suspect identified', c) other (please specify).”\n",
    "\n",
    "/* example question: */\n",
    "For award winners, which position has the most hall of fame players?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification questions: How should the 'position' for players be identified? a) by the `award_id` column, b) by the `category` column, c) other (please specify).\n",
    "user: c)  by the `note` column\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the answer should use the `note` column for player ‘positions’. However, it is unclear what fields should contain in the output table. Hence ‘AmbOutput’ remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “What fields should be contained in the output table? a) one field: the position, b) two fields: the position and the number of hall-of-fame players, c) other (please specify).”\n",
    "\n",
    "/* example question: */\n",
    "How many Wisconsin school districts receive federal funding?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: How to determine if a district has received federal funding? a) based on the t_fed_rev is larger than 0, b) the answer does not need to consider this aspect, c) other (please specify).\n",
    "user: c) every school in `FINREV_FED_17` table has received federal funding.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that every school in `FINREV_FED_17` table have received federal funding. However, it is unclear if the word ‘Wisconsin’ refers to the state or the school district. Hence, ‘AmbQuestion’ remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “Is 'Wisconsin school districts' referring to? a) all school districts in the state Wisconsin, b) school districts with names that contain Wisconsin, c) other (please specify).”\n",
    "\n",
    "/* example question: */\n",
    "How many 2-year public schools are there in \"California\"?\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: Which column(s) should be used to find ‘2-year public schools’? a) `level` column, b) `control` column, c) other (please specify).\n",
    "user: c) use both `level` and `control` columns to find ‘2-year public schools’ information.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the correct SQL answer should have a WHERE clause with filters based on the `level` and `control` columns. However, it is unclear what predicate values should be used for these two columns. Hence, ‘AmbValue’ remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “What predicate values should be used for the `level` and `control` columns to find  ‘2-year public schools’? a) ‘2-year’ and ‘public’ b) ‘2’ and ‘public, c) other (please specify)’.” \n",
    "\n",
    "/* example question: */\n",
    "Calculate the total beat of the crimes reported in a community area in the central side with a population of 50,000 and above.\n",
    "/* example previous clarification questions and user replies: */\n",
    "clarification question: What column and predicate value should be used to determine ‘central side’? a) Column `side` in table `Community_Area` with value ‘central’, b) Column `side` in table `Community_Area` with value ‘Central’, c) other (please specify).\n",
    "user: b) Column `side` in table `Community_Area` with value ‘Central’\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that the output table should contain a single number and use the predicate ‘Central’ in `Community_Area`.`side`; However, it is not clear which column of statistics is ‘total beat’ referring to. Hence, AmbTableColumn remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “Which column is related to ‘total beat’? a) `Crime`.`beat`, b) `Crime`.`report_no`, c) other (please specify).”\n",
    "\n",
    "/* example question: */\n",
    "Of all the nonessential genes that are not of the motorprotein class and whose phenotype is cell cycle defects, how many do not have a physical type of interaction?\n",
    "/* example previous clarification questions and user replies: */\n",
    "No previous clarification questions.\n",
    "/* example reasoning and remaining ambiguity type*/\n",
    "It is clear that ‘phenotype’ is referring to the `Phenotype` column, ‘motorprotein class’ is referring to the `class` column, ‘nonessential genes’ is referring to the `essential` column, and `physical type` is referring to the `type` column. However, it is unclear what fields should be contained in the output table, and hence ‘AmbOutput’ remains.\n",
    "/* example clarification question */\n",
    "mul_choice_cq = “What fields should be included in the output table? a) One column for the number of genes b) Two columns for GeneID and physical type c) Other (please specify).”\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "# SRA = \"\"\"Your task is to ask the user a single multiple choice clarification question to help you find the correct SQL query.\n",
    "# User does not understand SQL. The clarification question should be comprehensive by people with no coding experience.\n",
    "\n",
    "# QUESTION: ```{question}```\n",
    "# DATABASE SCHEMA:```{schema}```\n",
    "\n",
    "# Previous Clarification Questions and User Feedback: ```{cqs}```\n",
    "\n",
    "# List of Incorrect Queries: ```{sqls}```\n",
    "# The above SQL Queries are wrong.\n",
    "\n",
    "# Here are four ambiguity categories to consider:\n",
    "#     - AmbQuestion: Is the question itself ambiguous?\n",
    "#     - AmbTableColumn: Is there ambiguity in mapping the entities from QUESTION to tables and columns in DATABASE SCHEMA?\n",
    "#     - AmbOutput: What fields and how many fields should be contained in the output table?\n",
    "#     - AmbValue: What predicate value should be used to filter results?\n",
    "\n",
    "# Let's think step by step.\n",
    "\n",
    "# STEP 1, Summarize the information that is clear based on the ansewrs to previous clarification questions and incorrect queries.\n",
    "\n",
    "# STEP 2, Evaluate whether AmbQuestion, AmbColumn, AmbOutput, and AmbValue remain in formulating a SQL query to answer the QUESTION, considering each category individually.\n",
    "\n",
    "# STEP 3, Ask a multiple-choice clarification question to clarify the remaining ambiguities and help you find the correct SQL query. Ensure it's simple enough for someone without coding experience to understand.\n",
    "#         Use format: mul_choice_cq = \"\".\n",
    "        \n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# # decide when to stop\n",
    "# SRA_ES = \"\"\"Your task is to identify the remaining ambiguity.\n",
    "# If there are remaining ambiguities, then ask the user a single multiple choice clarification question to help you find the correct SQL query.\n",
    "# User does not understand SQL. The clarification question should be comprehensive by people with no coding experience.\n",
    "\n",
    "# QUESTION: ```{question}```\n",
    "# DATABASE SCHEMA:```{schema}```\n",
    "\n",
    "# Previous Clarification Questions and User Feedback: ```{cqs}```\n",
    "\n",
    "# List of Incorrect Queries: ```{sqls}```\n",
    "# The above SQL Queries are wrong.\n",
    "\n",
    "# Here are four ambiguity categories to consider:\n",
    "#     - AmbQuestion: Is the question itself ambiguous?\n",
    "#     - AmbTableColumn: Is there ambiguity in mapping the entities from QUESTION to tables and columns in DATABASE SCHEMA?\n",
    "#     - AmbOutput: What fields and how many fields should be contained in the output table?\n",
    "#     - AmbValue: What predicate value should be used to filter results?\n",
    "\n",
    "# Let's think step by step.\n",
    "\n",
    "# STEP 1, Summarize the information that is clear based on the ansewrs to previous clarification questions.\n",
    "\n",
    "# STEP 2, Evaluate whether AmbQuestion, AmbColumn, AmbOutput, and AmbValue remain in formulating a SQL query to correctly answer the QUESTION, considering each category individually.\n",
    "\n",
    "# STEP 3, If no remaining ambiguities are identified, then output \"NO AMBIGUITY\".\n",
    "#         Otherwise, ask a multiple-choice clarification question to clarify the remaining ambiguities and help you find the correct SQL query. Ensure it's simple enough for someone without coding experience to understand. Use format: mul_choice_cq = \"\".\n",
    "        \n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "SRA = \"\"\"/* Ask the user a new multiple choice clarification question to help you find the correct SQL answer for the following question: */\n",
    "{question}\n",
    "/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following previous clarification questions and user replies: */\n",
    "{cqs}\n",
    "\n",
    "/* Consider the following ambiguity categories:\n",
    "    - AmbQuestion: Is the question itself ambiguous?\n",
    "    - AmbTableColumn: Is there ambiguity in mapping the entities from the QUESTION to tables and columns in the DATABASE SCHEMA?\n",
    "    - AmbOutput: What fields and how many fields should be included in the output table?\n",
    "    - AmbValue: What predicate value should be used to filter results?\n",
    "*/\n",
    "\n",
    "/* The clarification question should be easy to understand for people with no coding experience. */\n",
    "\n",
    "/* Let's think step by step to generate the helpful multiple choice clarification question.\n",
    "1. Summarize the clear information based on previous clarification questions and incorrect queries.\n",
    "2. Evaluate whether AmbQuestion, AmbTableColumn, AmbOutput, and AmbValue remain in formulating an SQL query, considering each category individually.\n",
    "3. Ask a new multiple-choice question to address the remaining ambiguities and assist in identifying the correct SQL query. Use format: mul_choice_cq = \"\".\n",
    "*/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SRA_ES = \"\"\"/* Ask the user a new multiple choice clarification question to help you find the correct SQL answer for the following question: */\n",
    "{question}\n",
    "/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following previous clarification questions and user replies: */\n",
    "{cqs}\n",
    "\n",
    "/* Consider the following ambiguity categories:\n",
    "    - AmbQuestion: Is the question itself ambiguous?\n",
    "    - AmbTableColumn: Is there ambiguity in mapping the entities from the QUESTION to tables and columns in the DATABASE SCHEMA?\n",
    "    - AmbOutput: What fields and how many fields should be included in the output table?\n",
    "    - AmbValue: What predicate value should be used to filter results?\n",
    "*/\n",
    "\n",
    "/* The clarification question should be easy to understand for people with no coding experience. */\n",
    "\n",
    "/* Let's think step by step to generate the helpful multiple choice clarification question.\n",
    "1. Summarize the clear information based on previous clarification questions and incorrect queries.\n",
    "2. Evaluate whether AmbQuestion, AmbTableColumn, AmbOutput, and AmbValue remain in formulating an SQL query, considering each category individually.\n",
    "3. If no remaining ambiguities are identified, then output \"NO AMBIGUITY\".\n",
    "   Else, ask a new multiple-choice question to address the remaining ambiguities and assist in identifying the correct SQL query. Use format: mul_choice_cq = \"\".\n",
    "*/\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_generation = '''Complete sqlite SQL query only and with no explanation.\n",
    "# Your task is to write a SQLITE SQL QUERY that correctly answers the following QUESTION and align with database schema.\n",
    "\n",
    "# QUESTION: {question}\n",
    "\n",
    "# The DATABASE SCHEMA: {schema}\n",
    "\n",
    "# Answer:'''\n",
    "\n",
    "\n",
    "# sql_generation_v2 = '''Complete a SQLite SQL query only with no explanation.\n",
    "# Your task is to complete a new SQLite SQL QUERY that correctly answers the USER QUESTION, aligns with DATABASE SCHEMA, and follows USER FEEDBACK.\n",
    "\n",
    "# USER QUESTION: ```{question}```\n",
    "# The DATABASE SCHEMA: ```{schema}```\n",
    "\n",
    "# The user has provided a list of incorrect SQL queries and the following sql queries are wrong.\n",
    "# Incorrect Queries: ```[{sqls}]``` \n",
    "\n",
    "# The user has provided some feedback. User Feedback are the golden truth.\n",
    "# User Feedback: {cqas}\n",
    "\n",
    "# You know the Incorrect Queries are wrong. Do not rewrite the exact same incorrect queries.\n",
    "# Please listen carefully to the user feedback. Write the new correct query.\n",
    "\n",
    "# Answer:\n",
    "# '''\n",
    "\n",
    "# fix_invalid_v1 = \"\"\"Your task is to fix the invalid SQL query.\n",
    "\n",
    "# Database schema```{schema}```\n",
    "\n",
    "# Invalid SQL Query: ```{sql}```\n",
    "# The above SQL query is not valid and can not be executed.\n",
    "# Exception Message:```{ex}```\n",
    "\n",
    "# Fix the errors and output a new SQL query that is executable and align with Database schema.\n",
    "\n",
    "# Write the new executable SQL query in the format: sql = ``` ```.\n",
    "\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# sql_generation_selfdebug = '''Complete a SQLite SQL query only with no explanation.\n",
    "# Your task is to complete a new SQLite SQL QUERY that correctly answers the USER QUESTION, aligns with DATABASE SCHEMA, and follows USER FEEDBACK.\n",
    "\n",
    "# USER QUESTION: ```{question}```\n",
    "# The DATABASE SCHEMA: ```{schema}```\n",
    "\n",
    "# Incorrect Queries: ```[{sqls}]```\n",
    "# The above SQL predictions are wrong. Please fix the SQL.\n",
    "# Write the new correct query.\n",
    "\n",
    "# Answer:\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAIL SQLNoRule\n",
    "sql_generation = '''/* Given the following database schema: */\n",
    "{schema}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "SELECT '''\n",
    "\n",
    "# update to code representation\n",
    "sql_generation_v2 = '''/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* And the following user replies to help you write the correct sql query: */\n",
    "{cqas}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "SELECT '''\n",
    "\n",
    "# update to code representation\n",
    "fix_invalid_v1 = \"\"\"/* Given the following database schema: */\n",
    "{schema}`\n",
    "/* And the following inexecutable sql query */\n",
    "{invalidSQL}\n",
    "/* And the following exception message */\n",
    "{ex}\n",
    "\n",
    "/* Fix the exception and write a new executable SQL query with no explanation */\n",
    "SELECT \"\"\"\n",
    "\n",
    "# update to code representation\n",
    "sql_generation_selfdebug = '''/* Given the following database schema: */\n",
    "{schema}\n",
    "/* And the following incorrect sql answers: */\n",
    "{sqls}\n",
    "/* Answer the following with no explanation: {question} */\n",
    "SELECT '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_db_schema(database):\n",
    "    db_path = f'./bird_dev/dev_databases/{database}/{database}.sqlite'\n",
    "    conn = sqlite3.connect(db_path, uri=True)\n",
    "    full_schema_prompt_list = []\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = cursor.fetchall()\n",
    "    schemas = {}\n",
    "    for table in tables:\n",
    "        if table == 'sqlite_sequence':\n",
    "            continue\n",
    "        cursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{}';\".format(table[0]))\n",
    "        create_prompt = cursor.fetchone()[0]\n",
    "        schemas[table[0]] = create_prompt\n",
    "\n",
    "    for k, v in schemas.items():\n",
    "        full_schema_prompt_list.append(v)\n",
    "\n",
    "    schema_prompt = \"\\n\\n\".join(full_schema_prompt_list)\n",
    "\n",
    "    return schema_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startBenchBaseline(data_frame, history_log, log_name, rounds, num_of_tests, model_name, data_source):\n",
    "    assert model_name in ['gpt35turbo', 'gpt4turbo']\n",
    "    generation = None\n",
    "    if model_name == 'gpt35turbo':\n",
    "        generation = GPT35_generation\n",
    "    else:\n",
    "        generation = GPT4_turbo_generation\n",
    "    for index in range(num_of_tests):\n",
    "        if index in history_log and \"num_cq_asked\" in history_log[index]:\n",
    "            # skip tests already seen\n",
    "            continue\n",
    "        assert index in history_log\n",
    "        # we are using batch api to genearte the 0th round\n",
    "        # and read 0th sql from the log\n",
    "        assert len(history_log[index]['sql_log']) == 1\n",
    "        \n",
    "        cqs_and_answers = []\n",
    "        query = set()\n",
    "        d = data_frame.iloc[[index]] \n",
    "        if data_source == 'kaggle':\n",
    "            gold = d['sql'].values[0]\n",
    "            dbname = d['target_db'].values[0]\n",
    "            nlq = d['nl'].values[0]\n",
    "            dbschema = d['target_schema'].values[0]\n",
    "        elif data_source == 'bird':\n",
    "            gold = d['SQL'].values[0]\n",
    "            nlq = d['question'].values[0]\n",
    "            dbname = d['db_id'].values[0]\n",
    "            dbschema = generate_db_schema(dbname)\n",
    "        print(\"nl: \", nlq, index, dbname)\n",
    "        print(\"gold: \", gold)\n",
    "        \n",
    "#         sql_prompt = sql_generation.format(schema=dbschema, question=nlq)\n",
    "#         sql_query, pscore= generation(sql_prompt)\n",
    "#         history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "        order, sql_prompt, sql_query, pscore = history_log[index]['sql_log'][0]\n",
    "        order += 1\n",
    "        sql_query = clean_query(sql_query)\n",
    "        print(\"sql: \", sql_query, pscore)\n",
    "        query.add(sql_query)\n",
    "        execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "        if exception:\n",
    "            most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "            query.remove(most_recent_sql)\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                   invalidSQL=most_recent_sql, ex=exception[0])\n",
    "            sql, pscore= generation(invalid_prompt)\n",
    "            sql = clean_query(sql)\n",
    "            print(\"After Fix Invalid SQL: \", sql)\n",
    "            history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "            order += 1\n",
    "            query.add(sql)\n",
    "            execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "        if execution:\n",
    "            history_log[index]['num_cq_asked'] = 0\n",
    "            print()\n",
    "            print(\"-----execution match-----\")\n",
    "            print()\n",
    "            continue\n",
    "        for turn in range(rounds):\n",
    "            sql_prompt = sql_generation_selfdebug.format(schema=dbschema, question=nlq,\\\n",
    "                                              sqls=\",\\n\".join(query))\n",
    "            sql_query, pscore= generation(sql_prompt)\n",
    "            sql_query = clean_query(sql_query)\n",
    "            print(\"sql: \", sql_query, pscore)\n",
    "            history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "            order += 1\n",
    "            sql_query = clean_query(sql_query)\n",
    "            query.add(sql_query)\n",
    "            execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "            if exception:\n",
    "                most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "                query.remove(most_recent_sql)\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                     invalidSQL=most_recent_sql, ex=exception[0])\n",
    "                sql, pscore= generation(invalid_prompt)\n",
    "                sql = clean_query(sql)\n",
    "                query.add(sql)\n",
    "                history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "                order += 1\n",
    "                execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            if execution:\n",
    "                history_log[index]['num_cq_asked'] = turn + 1\n",
    "                print()\n",
    "                print(\"********execution match*********\")\n",
    "                print()\n",
    "                break\n",
    "        if 'num_cq_asked' not in history_log[index]:\n",
    "            history_log[index]['num_cq_asked'] = \"Failed\"\n",
    "        print('')\n",
    "        print(\"------next question------\")\n",
    "        print('')\n",
    "    save(log_name, history_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askClarificationQuestions(data_frame, history_log, log_name, rounds, num_of_tests, model_name, data_source):\n",
    "    assert model_name in ['gpt35turbo', 'gpt4turbo']\n",
    "    generation = None\n",
    "    if model_name == 'gpt35turbo':\n",
    "        generation = GPT35_generation\n",
    "    else:\n",
    "        generation = GPT4_turbo_generation\n",
    "    for index in range(num_of_tests):\n",
    "        if index in history_log and \"num_cq_asked\" in history_log[index]:\n",
    "            # skip tests already seen\n",
    "            continue\n",
    "        assert index in history_log\n",
    "        assert len(history_log[index]['sql_log']) == 1, print(index)\n",
    "        \n",
    "        d = data_frame.iloc[[index]] \n",
    "        cqs_and_answers = []\n",
    "        query = set()\n",
    "        if data_source == 'kaggle':\n",
    "            gold = d['sql'].values[0]\n",
    "            dbname = d['target_db'].values[0]\n",
    "            nlq = d['nl'].values[0]\n",
    "            dbschema = d['target_schema'].values[0]\n",
    "        elif data_source == 'bird':\n",
    "            gold = d['SQL'].values[0]\n",
    "            nlq = d['question'].values[0]\n",
    "            dbname = d['db_id'].values[0]\n",
    "            dbschema = generate_db_schema(dbname)\n",
    "        print(\"nl: \", nlq, index, dbname)\n",
    "        print(\"gold: \", gold)\n",
    "        \n",
    "        order, sql_prompt, sql_query, pscore = history_log[index]['sql_log'][0]\n",
    "        order += 1\n",
    "        sql_query = clean_query(sql_query)\n",
    "        print(\"sql: \", sql_query)\n",
    "        query.add(sql_query)\n",
    "        execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "        \n",
    "        if exception:\n",
    "            most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "            query.remove(most_recent_sql)\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                 invalidSQL=most_recent_sql, ex=exception[0])\n",
    "            sql, pscore= generation(invalid_prompt)\n",
    "            sql = clean_query(sql)\n",
    "            history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "            order += 1\n",
    "            query.add(sql)\n",
    "            execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "        if execution:\n",
    "            history_log[index]['num_cq_asked'] = 0\n",
    "            print()\n",
    "            print(\"-----execution match-----\")\n",
    "            print()\n",
    "            continue\n",
    "        for turn in range(rounds):\n",
    "            cqas = \"\"\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"multiple choice clarification question: \"+cqs_and_answers[i]+'\\n'\n",
    "                else:\n",
    "                    cqas += \"user: \"+cqs_and_answers[i]+'\\n'\n",
    "            if len(cqs_and_answers) == 0:\n",
    "                cqas = \"no previous clarification question.\\n\"\n",
    "            cq_prompt = SRA.format(schema=dbschema, question=nlq,\\\n",
    "                                            sqls=\",\\n\".join(query), cqs=cqas)\n",
    "            cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "            cq, pscore= generation(cq_prompt)\n",
    "            history_log[index]['cq_log'].append((order, cq_prompt, cq, pscore))\n",
    "            order += 1\n",
    "            print(\"cq: \", cq)\n",
    "            if \"mul_choice_cq = \" in cq:\n",
    "                cq = cq.split(\"mul_choice_cq = \")[-1]\n",
    "            \n",
    "            feedback_prompt = feedback_v2.format(query = gold, question = cq, nlq=nlq)\n",
    "            feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "            feedback, pscore= GPT4o_generation(feedback_prompt)\n",
    "            print()\n",
    "            print(\"feedback, \", feedback)\n",
    "            history_log[index]['feedback_log'].append((order, feedback_prompt, feedback, pscore))\n",
    "            order += 1\n",
    "            if \"answer_to_cq =\" in feedback:\n",
    "                feedback = feedback.split(\"answer_to_cq =\")[-1]\n",
    "            cqs_and_answers.append(cq)\n",
    "            cqs_and_answers.append(feedback)\n",
    "        \n",
    "            # fix incorrect sql based on user feedback\n",
    "            cqas = \"\"\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"multiple choice clarification question: \"+cqs_and_answers[i]+'\\n'\n",
    "                else:\n",
    "                    cqas += \"user: \"+cqs_and_answers[i]+'\\n'\n",
    "            if cqas == '':\n",
    "                cqas = 'no previous clarification questions are asked.\\n'\n",
    "            sql_prompt = sql_generation_v2.format(schema=dbschema, question=nlq,\\\n",
    "                                              sqls=\"\\n\".join(query), cqas=cqas)\n",
    "\n",
    "            sql_query, pscore= generation(sql_prompt)\n",
    "            sql_query = clean_query(sql_query)\n",
    "            print(\"sql: \", sql_query)\n",
    "            history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "            order += 1\n",
    "            query.add(sql_query)\n",
    "            execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "            if exception:\n",
    "                most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "                query.remove(most_recent_sql)\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                       invalidSQL=most_recent_sql, ex=exception[0])\n",
    "                sql, pscore= generation(invalid_prompt)\n",
    "                sql = clean_query(sql)\n",
    "                query.add(sql)\n",
    "                history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "                order += 1\n",
    "                execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            if execution:\n",
    "                history_log[index]['num_cq_asked'] = turn + 1\n",
    "                print()\n",
    "                print(\"********execution match*********\")\n",
    "                print()\n",
    "                break\n",
    "                \n",
    "        if 'num_cq_asked' not in history_log[index]:\n",
    "            history_log[index]['num_cq_asked'] = \"Failed\"\n",
    "        print('')\n",
    "        print(\"------next question------\")\n",
    "        print('')\n",
    "    save(log_name, history_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askCQsBreakNoAmb(data_frame, history_log, log_name, rounds, num_of_tests, model_name, data_source):\n",
    "    assert model_name in ['gpt35turbo', 'gpt4turbo']\n",
    "    generation = None\n",
    "    if model_name == 'gpt35turbo':\n",
    "        generation = GPT35_generation\n",
    "    else:\n",
    "        generation = GPT4_turbo_generation\n",
    "    for index in range(num_of_tests):\n",
    "        if index in history_log and \"num_cq_asked\" in history_log[index]:\n",
    "            # skip tests already seen\n",
    "            continue\n",
    "            \n",
    "        assert index in history_log\n",
    "        assert len(history_log[index]['sql_log']) == 1\n",
    "        d = data_frame.iloc[[index]] \n",
    "        cqs_and_answers = []\n",
    "        query = set()\n",
    "        if data_source == 'kaggle':\n",
    "            gold = d['sql'].values[0]\n",
    "            dbname = d['target_db'].values[0]\n",
    "            nlq = d['nl'].values[0]\n",
    "            dbschema = d['target_schema'].values[0]\n",
    "        elif data_source == 'bird':\n",
    "            gold = d['SQL'].values[0]\n",
    "            nlq = d['question'].values[0]\n",
    "            dbname = d['db_id'].values[0]\n",
    "            dbschema = generate_db_schema(dbname)\n",
    "        print(\"nl: \", nlq, index, dbname)\n",
    "        print(\"gold: \", gold)\n",
    "        \n",
    "        order, sql_prompt, sql_query, pscore = history_log[index]['sql_log'][0]\n",
    "        order += 1\n",
    "        sql_query = clean_query(sql_query)\n",
    "        print(\"sql: \", sql_query, pscore)\n",
    "        query.add(sql_query)\n",
    "        execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "        \n",
    "        if exception:\n",
    "            most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "            query.remove(most_recent_sql)\n",
    "            invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                 invalidSQL=most_recent_sql, ex=exception[0])\n",
    "            sql, pscore= generation(invalid_prompt)\n",
    "            sql = clean_query(sql)\n",
    "            history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "            order += 1\n",
    "            query.add(sql)\n",
    "            execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "        if execution:\n",
    "            history_log[index]['num_cq_asked'] = 0\n",
    "            print()\n",
    "            print(\"-----execution match-----\")\n",
    "            print()\n",
    "            continue\n",
    "        for turn in range(rounds):\n",
    "            cqas = \"\"\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"Multiple Choice Clarification Question: \"+cqs_and_answers[i]\n",
    "                else:\n",
    "                    cqas += \"Answer: \"+cqs_and_answers[i]\n",
    "            if cqas == '':\n",
    "                cqas = 'no previous clarification questions.\\n'\n",
    "                \n",
    "            cq_prompt = SRA_ES.format(schema=dbschema, question=nlq,\\\n",
    "                                            sqls=\",\\n\".join(query), cqs=cqas)\n",
    "            cq_prompt = cq_prefix_v1 + cq_prompt\n",
    "            cq, pscore= generation(cq_prompt)\n",
    "            history_log[index]['cq_log'].append((order, cq_prompt, cq, pscore))\n",
    "            order += 1\n",
    "#             print(\"cq: \", cq, pscore)\n",
    "            if \"NO AMBIGUITY\" in cq:\n",
    "                print()\n",
    "                print(\"-----NO AMBGUITY-----\")\n",
    "                print()\n",
    "                break\n",
    "            if \"mul_choice_cq = \" in cq:\n",
    "                cq = cq.split(\"mul_choice_cq = \")[-1]\n",
    "            \n",
    "            feedback_prompt = feedback_v2.format(query = gold, question = cq, nlq=nlq)\n",
    "            feedback_prompt = feedback_prefix_v1 + feedback_prompt\n",
    "            feedback, pscore= GPT4o_generation(feedback_prompt)\n",
    "#             print()\n",
    "#             print(\"feedback, \", feedback, pscore)\n",
    "            history_log[index]['feedback_log'].append((order, feedback_prompt, feedback, pscore))\n",
    "            order += 1\n",
    "            if \"answer_to_cq =\" in feedback:\n",
    "                feedback = feedback.split(\"answer_to_cq =\")[-1]\n",
    "            cqs_and_answers.append(cq)\n",
    "            cqs_and_answers.append(feedback)\n",
    "        \n",
    "            # fix incorrect sql based on user feedback\n",
    "            cqas = \"\"\n",
    "            for i in range(len(cqs_and_answers)):\n",
    "                if i%2 == 0:\n",
    "                    cqas += \"Multiple Choice Clarification Question: \"+cqs_and_answers[i]\n",
    "                else:\n",
    "                    cqas += \"User Feedback: \"+cqs_and_answers[i] \n",
    "            if cqas == '':\n",
    "                cqas = 'no previous clarification questions.\\n'\n",
    "            sql_prompt = sql_generation_v2.format(schema=dbschema, question=nlq,\\\n",
    "                                              sqls=\"\\n\".join(query), cqas=cqas)\n",
    "\n",
    "            sql_query, pscore= generation(sql_prompt)\n",
    "            print(\"sql: \", sql_query, pscore)\n",
    "            history_log[index]['sql_log'].append((order, sql_prompt, sql_query, pscore))\n",
    "            order += 1\n",
    "            sql_query = clean_query(sql_query)\n",
    "            query.add(sql_query)\n",
    "            execution, exception = evalfunc(sql_query, gold, dbname, data_source)\n",
    "            if exception:\n",
    "                most_recent_sql = clean_query(history_log[index]['sql_log'][-1][2])\n",
    "                query.remove(most_recent_sql)\n",
    "                invalid_prompt = fix_invalid_v1.format(schema=dbschema, question=nlq,\\\n",
    "                                                       invalidSQL=most_recent_sql, ex=exception[0])\n",
    "                sql, pscore= generation(invalid_prompt)\n",
    "                sql = clean_query(sql)\n",
    "                query.add(sql)\n",
    "                history_log[index]['sql_log'].append((order, invalid_prompt, sql, pscore))\n",
    "                order += 1\n",
    "                execution, _ = evalfunc(sql, gold, dbname, data_source)\n",
    "            if execution:\n",
    "                history_log[index]['num_cq_asked'] = turn + 1\n",
    "                print()\n",
    "                print(\"********execution match*********\")\n",
    "                print()\n",
    "                break\n",
    "                \n",
    "        if 'num_cq_asked' not in history_log[index]:\n",
    "            history_log[index]['num_cq_asked'] = \"Failed\"\n",
    "        print('')\n",
    "        print(\"------next question------\")\n",
    "        print('')\n",
    "    save(log_name, history_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3100",
   "language": "python",
   "name": "py3100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
